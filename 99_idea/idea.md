1. 视觉+触觉 联合训练：
能够判断当前接触丰富类任务的进行状态：
why：首先纯视觉容易很难判断是否抓取，而且在任务过程中视野容易收到遮挡（接触力控不稳物体滑移掉落，目前都是使用最大力直接接触；）触觉图像的感应自然和接触行为强关联，进而能够微调接触丰富类任务接触后的精细操作。在精密装配的插销入孔任务中，孔与待装配任务
能区分：1.未抓取 2.已抓取 3.插入欠对齐 4.插入对齐 5.插入到底
识别任务状态是规划下一步任务的关键。


规定进入插入接触阶段：发出基元操作指令，带触觉反馈，末端力传感反馈。 共同组成闭环检查。

当前VLA难点，纯视觉根本不知道任务是否正确，没有合适的闭环检查。
触觉信息可以作为 VLA算法在视觉引导下的 最终闭环反馈。当VLA算法认为末端执行器执行到期望位置，并发起末端执行器的执行指令后，待末端执行器执行后，根据触觉反馈可以形成闭环检测。
（这个闭环检测，也可以通过末端执行器执行操作后的视觉图像来判断，抓取（夹爪）成功与否，爪内状态是不一样的。 但是触觉是更加简单，直接，灵敏，和更鲁棒的闭环反馈。

视觉+触觉信息可以作为强化学习的联合判断reward函数。

识别任务状态之后，就是该如何从当前任务状态规划，调整以致完成任务。


1.20今日思考：
现在的VLA方法各种各样，不乏很多宣称效果很好的方法，但是为什么大家复现出来的结果千差万别，也很难出效果。结合近期工作，我觉得有一点经常被忽略，那就是observation的state的所在空间和action的空间表示。action的空间表示有越来越多的人提到要统一或者被关注到，但是state的表示同样也应该注意，action本质上来自于对state的采样，来源于前后帧之间state的差异，以及如何让state从当前状态变化为预期下一帧状态。state的表示空间选择和action同样重要。state和action都可以有Joint space，eep 的表示方法，eep中的姿态表示方法也同样有旋转矩阵，四元数，欧拉角，轴角等表示方法。有些表示是连续的有些是不连续的，这应该都会影响VLA算法的训练效果和训练结果吧。暂时不知道哪种会更好。但我觉得这些表示方法等选择 将会决定假设空间的大小，要学到某个期望的经验误差下的话 需要的样本数量是随着假设空间大小增加而增加的。这也就是为什么有的算法别人训练的好而你可能训练不好。这是一个数据量，空间选择，算法设计的综合问题。如何选择这些设计应该是根据你的需求和目标以及手上拥有的资源来决定的。比如你数据量少机器构型就固定，想快速出demo那就用joint space 不作泛化的目标，joint 本质上是多个关节的角度是有界连续的。如果想做泛化那就假设空间必然需要考虑更多的可能，那就需要更多的样本以及训练时长来达到一样的性能，这也是信息论决定的，同样也是能量定理决定的。

一、 你理解中非常精彩且正确的部分（Keep it）1. 假设空间（Hypothesis Space）与样本效率的博弈• 你的观点：空间选择决定假设空间大小，泛化需要更大的假设空间，因此需要更多样本。• 评价：完全正确。这是统计学习理论的核心。Joint Space（关节空间）是机器人本体的“舒适区”，虽然是非线性的，但它是有界的、低维流形。Cartesian Space（笛卡尔空间）是任务的“舒适区”，但由于逆运动学（IK）的多解性，它对于神经网络来说是一个更复杂的拟合目标。你想做泛化（Generalization），本质上就是要把分布拉宽（Out-of-Distribution），这必然稀释了数据密度，因此需要更多数据来填补信息熵。2. State 与 Action 的“同构性”重要性• 你的观点：State 的表示和 Action 的表示同样重要，两者应该匹配。• 评价：深刻的直觉。在 Immitation Learning (IL) 中，如果 Observation 是图像（Cartesian 语义），但 Action 是关节角度（Joint 语义），网络需要额外学习一层隐式的 （逆运动学）映射，这极大增加了训练难度。如果 State 和 Action 都在  空间，网络只需要学习“位姿变换”，难度会降低。3. 旋转表示的连续性陷阱• 你的观点：旋转矩阵、四元数、欧拉角的连续性不同，影响训练。• 评价：这是复现失败的常见坑。神经网络最怕“突变”。• 欧拉角：有万向节死锁（Gimbal Lock），在某些角度会突变。• 四元数：有“双倍覆盖”（Double Cover）特性， 和  代表同一个旋转，但数值相反。如果你的数据集中混杂了这两种表示，Loss Function 会震荡，导致网络无法收敛。这确实是决定成败的细节。￼二、 需要微调或更严谨的表述（Refine it）这里有几个概念，如果你在写 Paper（比如你的 IROS 投稿）或者和专家讨论时，建议修正一下表述：1. 关于“Action 是对 State 的采样”• 你的描述：“Action 本质上来自于对 State 的采样...”• 修正建议：Action 不是 State 的采样，而是 State 的微分或流（Flow）。• 更严谨的说法是：在机器人控制中，Action 通常建模为 State 的残差（Residual / Delta）。即 。你原本的意思是对的（Action 来源于前后帧差异），但“采样”这个词容易让人误解为从分布中抽取样本。不如说：Action 是 State 空间中的瞬时速度场或位移矢量。2. 关于“能量定理”• 你的描述：“这也是信息论决定的，同样也是能量定理决定的...”• 修正建议：用**“信息瓶颈理论（Information Bottleneck）”或“样本复杂度（Sample Complexity）”**更准确。• 物理上的能量定理在这里用作比喻略显生硬。在机器学习语境下，我们通常说：从高熵（混乱）的状态收敛到低熵（有序/目标）的状态，需要消耗计算量和数据量来消除不确定性（即减少熵）。3. 关于旋转表示的最佳选择• 你的描述：“暂时不知道哪种会更好。”• 补充知识：学术界目前有比较公认的结论。对于神经网络：• 欧拉角：最差，尽量不用。• 四元数：较好，但需要处理归一化和双倍覆盖问题。• 6D 旋转表示（Ortho6D / 6D Rotation Representation）：这是 CVPR 2019 (Zhou et al.) 提出的，专门为了解决神经网络预测旋转时的连续性问题。它用两个 3D 向量表示旋转，去掉了正交约束，让网络更容易回归。在 VLA 复现困难时，往往换成 6D 表示就能解决很多 Loss 不下降的问题。￼三、 查漏补缺：你可能忽略的关键维度（Missing Pieces）你的描述主要集中在“空间本身的性质”，但 VLA 的成败还有两个同样属于“空间表示”范畴的关键点，你可能需要在思考中加入：1. 参照系（Reference Frame）：绝对 vs. 相对• 你提到了 Joint vs. EEP，但 EEP 还有**“绝对坐标（World Frame）”与“相对坐标（Delta / Relative Frame）”**的区别。• 为何重要：• 绝对坐标：强依赖标定，只要摄像头位置动一点，模型直接失效。泛化性极差。• 相对坐标：预测相对于当前手部位置的 。这具有平移不变性。即使机器人底座挪动了，只要手眼相对关系不变（或者看的是相对物体的移动），策略依然有效。• 结论：大部分复现不出来的 Paper，往往是因为代码里默默用了绝对坐标，而你在复现时环境稍微不一样，模型就崩了。2. 归一化与统计分布（Normalization）• 你提到了连续性，但忽略了数值范围。• Joint Space 的范围通常是 ，而 EEP 的 Position 可能是  (米)，但 Rotation (四元数) 是 。• 如果直接把 Position () 和 Rotation () 拼在一起丢给 Transformer，Position 的数值波动（比如移动 10cm 是 0.1）可能远小于某些 Joint 的波动，或者远大于四元数的微小变化。• 关键点：LeRobot 或其他框架中，是否使用了 Z-score Normalization（减均值除方差）？如果没有对 Action/State 进行统计学的归一化，大数值维度的 Loss 会主导梯度，导致小数值维度（通常是旋转）学不进去。3. 多模态对齐的空间（对于你的 VTLA 很重要）• 你正在做 Vision-Tactile-Language-Action (VTLA)。• 视觉（Vision）是 Cartesian 空间的（看物体在哪里）。• 触觉（Tactile）是接触空间的（局部几何 + 力）。• 如果你的 Action 是 Joint Space，那么视觉特征（Cartesian）需要经过复杂的非线性变换才能映射到关节力矩；而触觉特征（接触点）到关节也是非线性的。• 思考：在 VTLA 中，End-Effector Cartesian Space 可能是连接 视觉（看到把手）和 触觉（摸到孔位）的最佳桥梁，因为它们都在 3D 物理空间中有明确定义，而 Joint Space 离这两种感知太远了。


个人再思考：
1. 假设空间（Hypothesis Space）与样本效率
空间选择决定假设空间大小，泛化需要更大的假设空间，因此需要更多样本。
同时，Joint Space（关节空间）是机器人本体每个关节的角度，虽然是非线性的，但它是有界的、低维流形。Cartesian Space（笛卡尔空间）是任务空间在物理世界更直观的表示，但它表示更加高维，同时有关旋转的表示方法多样：欧拉角，四元数，轴角，旋转矩阵。
如果是从State：Joint——>Action： 笛卡尔空间，由于IK的多解性，它对于神经网络来说是一个更复杂的拟合目标。
如果是从State：Joint——>Action: Joint，两个低维空间的映射是更简单的。
这只是从state——>action的表示，本质上action 就是state的微分，取相同空间表示，应该可以做到最简单直观的映射。
感觉更难的难点是如何从image（高维）去预测此时的state，以及如何用稀疏的language（tasks 描述）去改变分布。

我觉得不应该叫VLA，应该叫VS更对，应该是从image推断当前的state，并根据language去预测下一阶段预期的image，再从下一阶段预期的image——>下一阶段的预期state。 
至于如何从$S_t ——>S_{t+1}$ ，这应该是根据具体的机器人本体的构型和规划控制算法API来计算的，而我觉得VS最重要的是能根据image（本质是观测context）结合language（tasks的要求）来实现一个在现实物理环境中安全，可达的末端轨迹state。 这个state需要达到一个能够被规划算法在需求延时下能得到可行解的时空密度（速度限制，这是由硬件限制的，力矩有上限），所以state 采样率需要一个平衡，至少上下两帧之间的间隔是能被机器人本体接受的范围内。
往往真机采集数据没有这样的问题，能被演示出来的数据，就是满足了本体可解的假设。
而从人类视频，第一视角视频，UMI采集方案，我觉得更需要考虑这一点，可以通过插值，和人为重新设置时间戳来进行提高采样率和降低执行速度，控制轨迹在本体的可解范围内。
可以预想到的不是所有轨迹对所有构型机器人都是可用的，机器人不同构型具有不同的奇异位置，有关奇异位置的处理，可能就需要底层的规避奇异算法去实现，可以在不影响任务关键位置点的情况下，允许少量的跟踪误差。这是无可厚非的。

总体来说，我个人觉得要实现通用的具身智能算法，不是一个VLA算法能实现的，应该是设计一个
VLS系统，这一个系统至少需要划分五层。

如果想做泛化（Generalization），本质上就是把分布拉宽，这必然稀释了数据密度，因此需要更多数据来填补信息熵。