object should be noticed during in task "Place the cylinder into the circle hole in the first row in the robot view."

**input：**
state：joint space & 笛卡尔末端位姿 四元数表示位姿
image：3个visual相机：两个手部相机，一个正面俯视相机；两个视触觉相机：tactile image；
**output：**
action：当前：frame 的笛卡尔空间 末端位姿 （模型根据图片规划出末端的：trajectory）

action的采集后处理方式：
1.不处理：就用EE 的笛卡尔位姿(机器人base frame下的：绝对位姿， 会在进入网络前经过MEAN_STD 归一化处理)
2.增量 笛卡尔空间 四元数表示位姿势（用前后两帧的进行计算，同样的会经过MEAN_STD 归一化处理）
3.增量 欧拉角表示位姿

实验方案：都带state，state就只记录关节空间。action就是末端的笛卡尔空间。相当于这两个是求FK和IK的关系。（因为现有的文献都是带state 的，比较时候也必须要比较带state的，至少baseline是需要带上state保持与这些方法的原文是保持一致的）
目标：能够预测接下来的规划的末端位姿 笛卡尔空间的轨迹。 请问我这样是否合理。diffusion训练完能否按照我的想法进行预测。


Updated todo list

要点：
- 表示选择：
  - 位置：使用相对于机器人基座或工具坐标系的三维坐标，归一化/去量纲（减均值、除尺度）有助训练。归一化时候已按照MEAN_STD 进行归一化。
  - 姿态：优先使用单位四元数（q），训练时保持归一化并用四元数地理距离或角度损失；也可在切空间用旋转向量（axis-angle / Rodrigues）建模，噪声在切空间更合理。
  - 建议输出表示为 sequence of (x,y,z, quat) 或 sequence of (Δx,Δy,Δz, Δrot)；通常预测相对位移（delta）比直接预测绝对位姿更稳定、易泛化。
- 坐标系与条件信息：
  - 摄像头图像与动作必须在统一参考系（或提供相机到机器人变换）；若图像为原始像素，模型需学会基于视觉推断到机器人坐标，**最好把相机内外参作为输入或预校正到统一视角**。
  - 当前 state（关节角/末端位姿/速度等）作为条件一并输入（embedding）。
- 数据与标注：
  - 训练集需包含「当前（image, state） → 接下来若干步的真实末端位姿序列」。长度 N 与采样频率决定模型输出维度。
  - 多模态场景（多条可行轨迹）要保留多样性样本或使用随机初始化以学习多模态分布。
- diffusion 模型设计要点：
  - 条件 diffusion：输入为条件向量（图像特征 + state embedding）；噪声与去噪在轨迹向量空间（例如 $R^{(3+orient_{dim})*N}$  ）中进行。
  - Orientation 的噪声要在适当表示上添加（若用四元数，可在切空间对旋转向量加高斯噪声，然后映射回四元数）；直接在四元数分量上做 Gaussian 会破坏单位范数。
  - 网络骨干：时间序列模型（Transformer/Temporal U-Net/TCN/LSTM + cross-attention）常用；视觉特征可用 CNN or pretrained backbone，然后 cross-attention 到序列去噪器。
  - 损失：位置用 L2 或 Huber；姿态用旋转地理距离（angle between quaternions）或对旋转矩阵的 Frobenius 距离；也可结合任务损失（例如到目标的终端误差）。
  - 评价时可生成多样样本，选取最合适的（最低成本）或用 classifier-free guidance 提高目标导向性。
- 训练与推理实践：
  - 训练：大 batch，多样化条件，注意归一化与姿态约束。适当的噪声调度、较深的去噪网络与足够的训练数据对收敛重要。
  - 推理：采样若干轨迹（例如 8-32），计算每条轨迹的成本（碰撞、末端误差、平滑度等），选用最优；若需要确定性输出，可用预测器网络或取多次采样的均值/最小成本样本。
- 其它实用建议：
  - 若只关心「末端最终位姿」而非完整中间轨迹，可让模型只预测终点（维度更小，训练更稳定）；若需要全轨迹，分段预测或多尺度建模能提升质量。
  - 对姿态的数值稳定性做约束（四元数重归一化、损失中加入正则）。
  - 在仿真/真实场景上做域随机化、图像增强，提升视觉条件的鲁棒性。




已把计划改成“VLA + Language 指令条件 + 预训练 backbone + Diffusion/FlowMatch 动作头”的版本，并显式按你说的 **absolute ee_pose、base 坐标系** 来设计。

**1) 定义任务/坐标系与语言规范**
- **条件输入**：$(I^{vision}_t, I^{tactile}_t, s_t, I^{language\_instruction}_t)$ 
- **输出**：未来 $H$ 帧末端位姿（absolute，base frame）：$y=\{T^{ee}_{t+1},...,T^{ee}_{t+H}\}$
- **指令形式**：
  - 最简单：每条轨迹一个 `task_description` 字符串
  - 更强：结构化指令（目标物体、目标位姿、约束）→ 仍可序列化成文本
- **多模态缺失策略**：tactile 可缺失（mask）；语言必须存在或提供 “null instruction”
- **建议（不强制）**：训练内部可以用 delta 表示去噪/回归，但最终再积分回 absolute（对 diffusion/flow 更稳）

**2) 实现含 Language 的数据管线与对齐**
- 数据集样本最小字段：
  - `vision_image[t]`, `tactile_image[t]`, `state[t]`, `ee_pose[t+1:t+H]`, `instruction`
- **时间对齐**：确保图像/触觉/状态对齐到同一时刻 t（必要时插值 state）
- **tokenization & cache**：
  - 用预训练 tokenizer（取决于你选的 text encoder）
  - 训练时建议缓存 `text_embedding`（指令通常重复，省大量算力）

**3) 接入预训练 Vision/Language 骨干（你希望的“任务引导关注”关键）**
你有两条可落地路线（推荐先 A，再升级 B）：
- **A. 预训练 Vision backbone + 预训练 Text encoder（最常见、易训练）**
  - Vision：ViT/ConvNeXt/CLIP-vision（冻结或 LoRA）
  - Text：CLIP-text / T5 / LLaMA-like encoder（通常先冻结或 LoRA）
  - 输出 tokens：保留 patch tokens（不要只取 pooled embedding），方便“把注意力分配到图像区域”
- **B. 直接用 VLM（如 LLaVA 类）做视觉-语言对齐（工程更重）**
  - 好处：指令-视觉绑定更强
  - 代价：训练成本、显存、数据量要求更高

**4) 语言引导的跨模态融合（把“注意力”真正用起来）**
目标：让 text 作为 Query 去选择“图像中与任务相关的 token”。
- **推荐结构（清晰、可控）**：
  1) `Text tokens` → cross-attn 到 `Vision tokens`（Text→Vision），得到任务相关视觉表示
  2) `Text tokens` → cross-attn 到 `Tactile tokens`（Text→Tactile），得到任务相关触觉表示
  3) 再做 `Vision↔Tactile` 双向 cross-attn（可选，但你想要就加）
  4) 融合后与 `state token` 拼接，送入动作头条件网络
- 工程细节：
  - modality embedding + attention mask（支持缺失 tactile）
  - token 数控制：对 vision/tactile tokens 做 pooling 或用 Perceiver bottleneck，避免 $O(N^2)$ 爆炸
- 可选增强：
  - **classifier-free guidance**：训练时随机丢弃 text/vision 条件，推理时调 guidance 强度，让“按指令”更强

**5) 轨迹动作头（Diffusion Policy 或 Flow Matching）**
输出是未来 $H$ 帧的位姿序列（absolute/base frame）。
- **Diffusion Policy 头**
  - 在轨迹向量 $y$ 上做加噪去噪；条件为融合特征 $c$
  - 重点：姿态部分的噪声/损失要在合理表示空间（建议 axis-angle/tangent space），而不是直接对四元数分量加高斯
- **Flow Matching（Rectified Flow / FM）头**
  - 学速度场 $v_\theta(y,t,c)$ 或直接学从噪声到数据的 flow
  - 推理 ODE/SDE 求解，通常比 diffusion steps 更可控、更快（实现也不算复杂）
- 两者共通：可以生成多条候选轨迹，然后按代价函数挑选最优

**6) 训练目标：监督 + 几何约束 + 指令一致**
- **轨迹监督**：
  - 位置：Huber/L2
  - 姿态：角度距离（或 rotvec L2）
  - 时间加权：终点更重要（planning 常见）
- **平滑/可执行约束**：速度/加速度正则（避免抖动）
- **指令一致性（让语言真的起作用）**：
  - 最简单有效：做 CFG + 验证“换指令输出会变”
  - 可选：引入轻量对比学习（text embedding 与“任务相关视觉 token 聚合”对齐），但别让它抢主任务

**7) 推理：指令条件采样与轨迹选择**
- 输入：当前观测 + 指令
- 输出：采样 $N$ 条轨迹（N=1/8/16），选择：
  - 若有简单代价（终点接近目标、平滑、约束），选最小代价
  - 或用一个 learned value / critic 做 rerank（后续再加）

**8) 评估：终点/轨迹误差 + 指令泛化**
- 基础：逐帧位置误差、姿态角误差、终点误差、平滑度
- 指令相关：
  - 同一观测换不同指令，输出是否有区分（定性+定量）
  - 未见过的指令组合/同义表达鲁棒性
- 如果你有多任务数据：做 per-task 成功率或终点达成率

**9) 工程化：配置/缓存/复现实验**
- 配置项：`H、K、vision_backbone、text_encoder、use_tactile、fusion_layers、action_head(diffusion/fm)、guidance_scale、num_samples`
- 缓存：text embeddings、(可选) vision features（如果冻结 backbone）
- 实验记录：wandb/tensorboard、checkpoint、可复现实验 seed

你接下来只需要再确认 3 个关键选择，我就能把“网络结构图 + 模块实现清单”进一步细化到你现有代码/脚本里：
1) 指令是 **每条 episode 固定**，还是 **每个时间步可能变化**？
2) 触觉图像是 **单张** 还是 **多传感器多张**？分辨率大概多少？
3) 动作头你更倾向先落地 **Diffusion** 还是 **Flow Matching**（以及你希望的推理速度目标）？