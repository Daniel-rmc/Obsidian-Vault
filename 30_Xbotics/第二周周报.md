# 📑 学习周报：强化学习基础与 Q-Learning 算法

日期：2026-01-18

主题：RL 基础理论 / MDP / Q-Learning

学员：Daniel_rmc

学习时长：8h

## 1. 核心概念

强化学习的本质是**智能体 (Agent)** 通过与 **环境 (Environment)** 交互，通过试错学习以最大化**累积奖励 (Return)** 的过程。

- **基本要素**：
    
    - **状态 ($S$)**：Agent 所处的环境情境 (Context)。
        
    - **动作 ($A$)**：Agent 在当前状态下能做出的行为。
        
    - **奖励 ($R$)**：环境给出的**即时**反馈包含奖励，惩罚，以及一项平凡的反馈（+100, -10, 0）。
        
    - **策略 ($\pi$)**：Agent 的行为策略，即 $S \rightarrow A$ 的映射。
        
- 奖励和价值：
    
    - **奖励 (Reward) vs 价值 (Value)**：
        
        - Reward 是当下的反馈。
            
        - Value 是**长远的眼光**，考虑了未来的累积收益。
            
    - **折扣因子 ($\gamma$)**：决定了 Agent 有多“远视”。$\gamma \approx 1$ 代表重视未来，$\gamma = 0$ 代表短视。

## 2. 数学模型：马尔可夫决策过程 (MDP)

一般将一个强化学习问题形式化为一个五元组：$\mathcal{M} = \langle S, A, P, R, \gamma \rangle$。

### 核心假设：马尔可夫性质 (Markov Property)

> "未来只取决于现在，与过去无关。"
> 
> $P(S_{t+1} | S_t) = P(S_{t+1} | S_t, S_{t-1}, ...)$

### 贝尔曼方程 (The Bellman Equation)

这是 RL 的灵魂公式，描述了**当前价值**与**未来价值**的迭代关系：

$$Q(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \max_{a'} Q(s', a')$$

- **物理含义**：动作 $a$ 的价值 = 即时奖励 + (折扣后) 下一个状态可能获得的**最大**价值的期望。

## 3. 算法实现：Q-Learning

Q-Learning 是一种 **Off-policy** (离线策略) 算法，核心思想是维护一张 **Q-Table**。

### 3.1 核心更新公式

Agent 基于**时序差分 (Temporal Difference, TD)** 误差来更新记忆：

$$Q(s,a) \leftarrow Q(s,a) + \alpha [ \underbrace{r + \gamma \max_{a'} Q(s', a')}_{\text{TD Target (现实)}} - \underbrace{Q(s,a)}_{\text{Current (记忆)}} ]$$

- **$\alpha$ (学习率)**：决定了我们要多大程度上接受新的事实（覆盖旧记忆）。
- $[ \underbrace{r + \gamma \max_{a'} Q(s', a')}_{\text{TD Target (现实)}} - \underbrace{Q(s,a)}_{\text{Current (记忆)}} ]$ 这一项又常被称作“惊喜”。

### 3.2 探索与利用

为了避免陷入局部最优，引入 **$\epsilon$-Greedy** 策略：

- **$\epsilon$ 概率**：随机乱走 ，为了发现潜在的高分路径。
    
- **$1-\epsilon$ 概率**：查表选最大值 ，利用已有经验。
    
- **Decay**：随着训练进行，$\epsilon$ 逐渐减小，从“莽撞”变得“稳重”。

### 3.3 伪代码

Python

```
Initialize Q_table with zeros
For episode in range(Max_Episodes):
    State = env.reset()
    While not Done:
        # 1. 决策：Epsilon-Greedy
        if random < epsilon:
            Action = random_choice()
        else:
            Action = argmax(Q_table[State])
            
        # 2. 交互：执行动作
        Next_State, Reward, Done = env.step(Action)
        
        # 3. 学习：更新 Q 表
        Target = Reward + gamma * max(Q_table[Next_State])
        Q_table[State, Action] += alpha * (Target - Q_table[State, Action])
        
        # 4. 迭代
        State = Next_State
```

## 4.简单的Q-learning实验内容
### 4.1. 实验内容
在 `FrozenLake-v1` (4x4 网格) 环境中，开启 **`is_slippery=True`** (模拟现实世界的摩擦力/误差)，进行了 **10,000 次** Q-Learning 训练。
### 4.2.代码
```
import gymnasium as gym

import numpy as np

import time

import random

  

# --- 1. 升级版超参数 ---

LEARNING_RATE = 0.1 # 调低：在滑溜溜的冰面上，不能学得太急

DISCOUNT_FACTOR = 0.99 # 调高：更加重视长远的终点奖励

EPISODES = 10000 # 加量：练个一万次！

IS_SLIPPERY = True # 开启困难模式 (模拟真实世界的误差)

  

# Epsilon 衰减策略 (从 1.0 慢慢降到 0.05)

epsilon = 1.0

max_epsilon = 1.0

min_epsilon = 0.05

decay_rate = 0.0005 # 衰减速度

  

# --- 2. 初始化环境 ---

env = gym.make('FrozenLake-v1', map_name="4x4", is_slippery=IS_SLIPPERY, render_mode=None)

  

# --- 3. 初始化 Q 表 ---

Q_table = np.zeros((env.observation_space.n, env.action_space.n))

  

print(f"🚀 开始强化训练 ({EPISODES} 次)... 这次会久一点")

  

# --- 4. 训练循环 (带监控版) ---

print(f"🚀 开始强化训练 ({EPISODES} 次)...")

print("我们将每隔 2000 次打印一次 Q 表的前 5 行，请观察数值的变化！\n")

  

for episode in range(EPISODES):

state, info = env.reset()

done = False

while not done:

# 策略

if random.uniform(0, 1) < epsilon:

action = env.action_space.sample()

else:

action = np.argmax(Q_table[state, :])

  

# 交互

next_state, reward, terminated, truncated, _ = env.step(action)

done = terminated or truncated

  

# 更新

q_target = reward + DISCOUNT_FACTOR * np.max(Q_table[next_state, :])

Q_table[state, action] += LEARNING_RATE * (q_target - Q_table[state, action])

  

state = next_state

# 衰减 epsilon

epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)

  

# --- 🔥 新增：监控代码 ---

# 每 2000 次，打印一次快照

if (episode + 1) % 2000 == 0:

print(f"--- 📅 训练进度: {episode + 1}/{EPISODES} ---")

print("状态 0 (起点) 到 状态 4 的 Q 值:")

print(np.round(Q_table[:5, :], 3)) # 只打印前5行，保留3位小数

print("-" * 30)

  

print("✅ 训练完成！")

  

print("\n最终生成的 Q-Table (部分预览):")

print(np.round(Q_table[:5, :], 3)) # 保留3位小数打印，看着舒服点

  

# --- 5. 成果展示 ---

print("\n🎥 正在演示训练成果 (看弹窗)...")

env.close()

  

# 演示 5 次，看看能成功几次

env = gym.make('FrozenLake-v1', map_name="4x4", is_slippery=IS_SLIPPERY, render_mode="human")

  

for i in range(3):

print(f"\n🎬 演示第 {i+1} 局:")

state, info = env.reset()

done = False

time.sleep(1)

  

while not done:

action = np.argmax(Q_table[state, :])

next_state, reward, terminated, truncated, _ = env.step(action)

done = terminated or truncated

state = next_state

# time.sleep(0.1) # 动作快一点，不然滑来滑去太慢了

  

if reward == 1:

print("🏆 成功！(这就是 RL 的力量)")

else:

print("☠️ 失败... (滑倒了)")

  

env.close()
```
### 4.3.运行结果：
🚀 开始强化训练 (10000 次)...
每隔 2000 次打印一次 Q 表的前 5 行，请观察数值的变化！

--- 📅 训练进度: 2000/10000 ---
状态 0 (起点) 到 状态 4 的 Q 值:
[[0.507 0.476 0.487 0.472]
 [0.266 0.35  0.27  0.45 ]
 [0.359 0.314 0.291 0.389]
 [0.15  0.157 0.209 0.33 ]
 [0.522 0.368 0.368 0.368]]
------------------------------
--- 📅 训练进度: 4000/10000 ---
状态 0 (起点) 到 状态 4 的 Q 值:
[[0.563 0.547 0.557 0.554]
 [0.451 0.431 0.335 0.527]
 [0.429 0.416 0.414 0.489]
 [0.271 0.298 0.302 0.472]
 [0.576 0.396 0.509 0.442]]
------------------------------
--- 📅 训练进度: 6000/10000 ---
状态 0 (起点) 到 状态 4 的 Q 值:
[[0.572 0.545 0.551 0.556]
 [0.385 0.313 0.318 0.508]
 [0.438 0.43  0.415 0.464]
 [0.368 0.33  0.292 0.444]
 [0.593 0.437 0.354 0.46 ]]
------------------------------
--- 📅 训练进度: 8000/10000 ---
状态 0 (起点) 到 状态 4 的 Q 值:
[[0.552 0.532 0.518 0.52 ]
 [0.36  0.35  0.28  0.504]
 [0.43  0.422 0.412 0.462]
 [0.222 0.246 0.286 0.442]
 [0.566 0.427 0.467 0.293]]
------------------------------
--- 📅 训练进度: 10000/10000 ---
状态 0 (起点) 到 状态 4 的 Q 值:
[[0.533 0.493 0.506 0.479]
 [0.302 0.299 0.288 0.475]
 [0.391 0.401 0.394 0.439]
 [0.333 0.326 0.357 0.418]
 [0.552 0.437 0.305 0.388]]
------------------------------
✅ 训练完成！
最终生成的 Q-Table (部分预览):
[[0.533 0.493 0.506 0.479]
 [0.302 0.299 0.288 0.475]
 [0.391 0.401 0.394 0.439]
 [0.333 0.326 0.357 0.418]
 [0.552 0.437 0.305 0.388]]

🎥 正在演示训练成果 (看弹窗)...

🎬 演示第 1 局:
☠️ 失败... (滑倒了)

🎬 演示第 2 局:
🏆 成功！(这就是 RL 的力量)
![[截屏2026-01-18 22.09.14.png]]
![[截屏2026-01-18 22.09.20.png]]
通过这个代码，深入体会了Q-learning中的Q表如何进行更新。并成功通过强化学习学会了找宝藏。
### 4.4. 实验数据分析

通过监控 Q-Table 在训练过程中的演变（Episode 2k -> 10k），得出以下结论：

- **价值传导 **：
    
    - 观察到 Q 值从终点 (Goal) 逐渐向起点 (Start) 倒推传播。
        
    - **现象**：起点的 Q 值从 2000 次时的 `0.445` 上升并收敛至 10000 次时的 `~0.57`。
        
    - **结论**：Q-learning算法成功建立了从起点到终点的因果链条，机器人“意识”到了远处的奖励。

- **对随机性的适应 ：
    
    - **现象**：收敛后的 Q 值并非理想状态的 `1.0`，而是 `0.57` 左右。且演示中偶有失败（Demo 第3局）。
        
    - **分析**：这并非算法未收敛，这是因为我们设置了环境物理特性的真实反映，设置为了困难模式。在打滑环境下，即使执行最优策略，存活率也仅有 57%。
        
    - 映射到现实中的意义：这模拟了现实中的Agent在执行任务时，因各种传感器噪声或传动误差导致的“非确定性”。RL 学习到的是**期望价值，而非绝对成功。

- **策略极化：
    
    - **现象**：在关键状态（如 State 1），最优动作的 Q 值 (`0.529`) 远高于其他动作 (`~0.2`)。
        
    - **结论**：模型形成了鲜明的风险厌恶策略，学会了在危险边缘选择容错率最高的动作（如“贴墙走”）。


## 5. 痛点分析：Q-Learning 的局限性

虽然 Q-Learning 在迷宫中表现出色，但在实际现实中的机械臂 + 视觉 VLA算法中会面临**维度灾难**：

1. **状态空间爆炸**：
    - Q-Learning 依赖表格存储经验。
        
    - 上述小实验的迷宫状态仅 16 个，表格极小。
        
    - 机器人的输入为 RGB 图像 (例如 图像为224x224 像素)，状态数量高达 $256^{224 \times 224 \times 3}$。
        
    - **结论**：全宇宙的原子加起来都不够存这张 Q 表。
        
2. **缺乏泛化能力**：
    - 表格型 RL 必须遍历过每一个状态才能填表。
        
    - 对于机器人，摄像头的姿态稍微偏转 1 度就是一个“新状态”。查表法遇到没见过的画面（新的一行）只能瞎猜（Q=0）。
Q-Table 只能处理离散的、状态空间较小的问题（如迷宫）；为了解决上述问题，必须从记忆表格 转向函数拟合。

3. 下一步学习计划：引入 Deep Q-Network (DQN)

- **核心思路**：用一个神经网络替代 Q-Table。
    
- **输入**：机器人的视觉图像或传感器数据。
    
- **输出**：各动作的 Q 值。
    
- **优势**：
    - **压缩空间**：不需要存几亿行表格，只需要存储几兆的权重参数。
        
    - **泛化能力**：神经网络可以理解图像特征。即使遇到从未见过的具体像素排列，只要特征相似（比如都看到了“红色的孔”），它也能预测出相似的 Q 值。
## 6. 学习心得

- **心得**：Q-Learning 就像是一个拥有上帝视角的Excel表格的填空游戏。它不需要预先知道环境的物理规则 ($P$)，这使得它非常强大。
    
- **局限性**：Q-Table 只能处理离散的、状态空间较小的问题（如迷宫）。

- **下一步学习计划**：
    - **学习目标**：DQN —— 用神经网络替代 Q-Table。

另外还有一些跟机器人相关的强化学习的心得领悟如下：

对于机器人来说，从零开始试错（从头纯 RL训练）通常非常缓慢且危险。**模仿学习**提供了一个捷径：利用专家提供的**演示数据集**来训练策略。
• **行为克隆 (Behavior Cloning, BC)**：BC是最简单的机器人学习方法，直接把模仿专家演示数据集看作是一个监督学习任务，让模型回归到专家的动作上。
• **复合误差 (Compounding Errors)**：模仿学习的一个局限性是：模型在运行中一旦产生微小偏差，就会进入专家从未见过的状态。由于没学过如何从错误状态恢复，这些误差会迅速累积导致失败。
• **DAgger 算法**：为了解决上述问题，DAgger 算法被提出，它让机器人在真实环境中运行基策略$\pi_{base}$ ，然后请专家对机器人遇到的新情况进行实时“指导”标注，从而教模型如何纠偏。

另外为了能更好的纠正机器人在现实场景中的动作轨迹，应该引入human in the loop的强化学习框架，由于机器人在现实物理世界中进行探索会很危险，当动作超出规定范围或者产生碰撞 会对环境或机器人本地造成难以估量的损失，因此human in the loop的人类在探索中介入和监督的加入是必要的，这可以在训练完一个基础base策略后，让机器人在规定的大概范围内进行探索，然后当出现轨迹偏离预想的时候，人类就通过遥操作方式介入策略，此时基策略同时生成action表示，但是机械臂此时被人类的控制接管，同时记录下来这段修正轨迹，将人类介入前后的轨迹进行拼接，作为一个新采集的数据集，这段数据可以加入强化学习后续的学习更新中，这样可以让机器人又安全又快速的在现实场景下完成强化学习。
相关的工作我调研到有：HIL-SERL: Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning